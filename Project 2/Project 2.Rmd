---
title: "Project 2"
author: "Anna Brown, Rishika Deshmukh, and Ada Zhang"
date: "`r format(Sys.Date(), '%B %d, %Y')`"
output: 
  html_document:
    toc: true        # Enable the Table of Contents
    toc_depth: 3     # Set the depth of headings included in the TOC
    toc_float: true  # Enable floating TOC (keeps TOC fixed as you scroll)
    code_folding: none
    self_contained: false
    css: "styles.css"  # Link to an external stylesheet
---

```{r echo=FALSE, warning=FALSE, message=FALSE, results="hide"}
# Function to check and install missing packages
install_if_missing <- function(packages) {
  missing_packages <- packages[!packages %in% installed.packages()[, "Package"]]
  if (length(missing_packages) > 0) {
    install.packages(missing_packages, dependencies = TRUE)
  }
  invisible(lapply(packages, library, character.only = TRUE))
}

# List of required packages
packages <- c(
  "countrycode", "dplyr", "DT", "ggplot2", "gridExtra", "leaflet",
  "maps", "patchwork", "readr", "rworldmap", "showtext",
  "sp", "tidyverse", "stringr", "syuzhet"
)

# Ensure all packages are installed and loaded
install_if_missing(packages)
```

```{r include=FALSE}
# Enable the showtext package
showtext_auto()

# Add Google Fonts
font_add_google("Bodoni Moda", "bodoni_moda")  # Alias: bodoni_moda
font_add_google("Montserrat", "montserrat")     # Alias: montserrat
```

# **Introduction**
This project aims to analyze the tone of articles over a significant period by scrutinizing key words in datasets of articles published by The New York Times. The focus period spans from January 1, 1997, to December 31, 2022, a timeframe that captures several legislative and economic shifts influencing U.S. work visa programs.

A pivotal aspect of our analysis involves exploring the correlation between the volume of articles and the sentiment conveyed within them. Specifically, we aim to determine whether an increase in the quantity of articles about U.S. work visa programs published by The New York Times correlates with a more negative portrayal of these topics. This question is significant as it can indicate whether media coverage tends to become more negative as the frequency of articles on a subject increases, potentially reflecting broader public and political sentiments during periods of intensified discussion.

# **Background**

## Skilled Labor Visas  

The **H-1B visa** is the most prominent skilled labor visa, allowing U.S. employers to hire highly skilled foreign professionals for specialty occupations requiring advanced knowledge and at least a bachelor’s degree or its equivalent. Over the years, the H-1B program has been influenced by economic and political factors. For instance, in 1998, high-tech industry lobbying led to proposals to increase the H-1B visa cap, citing a significant unmet demand for skilled workers. In response, the American Competitiveness in the 21st Century Act temporarily raised the cap to 195,000 between 2001 and 2003, before reverting to 65,000 in 2004. The 2008 recession brought further scrutiny, with Congress targeting the program to protect domestic jobs, resulting in a decline in filed petitions. These fluctuations underscore the program’s sensitivity to broader economic conditions and labor market demands.  


## Economic Perspectives on Foreign Workers

Economic discussions about foreign workers revolve around their contributions to growth and innovation as well as concerns about their impact on the domestic labor market. Proponents argue that foreign workers fill critical labor shortages, enhance innovation, and sustain industries with fluctuating labor demands. For example, H-1B visa holders are often associated with technological advancements, patents, and entrepreneurial activities that drive competitiveness. 

Critics, however, caution against potential wage suppression and increased competition for domestic workers, emphasizing the need for careful policy design to ensure equitable benefits. This research delves into these dynamics, examining how U.S. visa policies reflect broader economic priorities and labor market realities while balancing domestic interests with international interdependence.

# **Methodology**

## Data Sources

**Data Source Introduction**

Our project draws curated datasets of articles published by The New York Times between January 1, 1997, and December 31, 2022, focusing on U.S. work visa programs. These datasets highlights coverage of skilled work visas (H-1B), providing valuable insights into labor market dynamics and immigration policies over the past 25 years.

To construct the datasets, our team utilized **Nexis Uni** to filter and extract relevant articles.Our team applied the following parameters:


| Parameter       | Description                                          |
|-----------------|------------------------------------------------------|
| Keywords        | "U.S visa", "H1-B"                 |
| Source Type     | Newspapers                                           |
| Source Location | North America, United States                         |
| Language        | English                                              |
| Timeline        | January 1, 1997 – December 31, 2022                  |
| Source Name     | The New York Times                                   |



The filtered articles were categorized by the H-1B visa type and saved into a text file based on their date of publication. This  approach allows for easier analysis of trends and policy discussions within each category over the years. 

# **Exploratory Analysis**



```{r display-image, echo=FALSE}

# Use knitr to include an external image
#knitr::include_graphics("code.png")

```

```{r include=FALSE}

# Function to load, process, and write documents to separate files
load_and_process_data <- function(url, source_label) {
  content <- readLines(url, warn = FALSE)
  text <- paste(content, collapse = "\n")
  documents <- str_split(text, "End of Document", simplify = FALSE)[[1]]

  # Write each document to a separate file
  for (i in seq_along(documents)) {
    doc_path <- paste0(source_label, "_Document_", i, ".txt")
    writeLines(str_trim(documents[i]), con = doc_path)
    cat("Document saved:", doc_path, "\n")
  }
}

# Load data from New York Times
load_and_process_data("https://github.com/annab0503/DS4002/blob/main/Project%202/Skilled%20Worker%20Visas.txt?raw=true", "NYT")

# Load data from San Diego Tribune
load_and_process_data("https://github.com/annab0503/DS4002/blob/main/Project%202/.txt%20files/San%20Diego%20Tribune.txt?raw=true", "SDT")

# Load data from USA Today
load_and_process_data("https://github.com/annab0503/DS4002/blob/main/Project%202/.txt%20files/USA%20Today.txt?raw=true", "USA")


```



```{r include=FALSE}

# Step 2: Load Split Documents
doc_files <- list.files(pattern = "^(NYT_|SDT_|USA_)Document_\\d+\\.txt")

# Read all documents into a list
documents <- lapply(doc_files, readLines)

# Combine each document's content into a single text string
documents <- lapply(documents, paste, collapse = " ")

# Create a data frame for analysis
doc_data <- data.frame(
  Document = doc_files,
  Text = unlist(documents),
  stringsAsFactors = FALSE
)


```

```{r include=FALSE}


# Step 3: Extract Dates from the Text
extract_date <- function(text) {
  date_pattern <- "\\b(January|February|March|April|May|June|July|August|September|October|November|December) \\d{1,2}, \\d{4}\\b"
  date <- str_extract(text, date_pattern)
  if (is.na(date)) {
    cat("Date extraction failed for document:", substr(text, 1, 100), "...\n")
  }
  return(date)
}

# Apply the function to extract dates for all documents
doc_data$Date <- sapply(doc_data$Text, extract_date)
doc_data$Date <- as.Date(doc_data$Date, format = "%B %d, %Y")
doc_data$Year <- format(doc_data$Date, "%Y")

# Display a preview of the dataset
head(doc_data[, c("Document", "Date", "Year")])

```

```{r echo=FALSE, warning=FALSE}
library(tidyr)

#president mentions
presidents <- c("Biden", "Trump", "Obama", "Bush")


count_presidents <- function(text, presidents) {
  sapply(presidents, function(president) {
    str_count(text, president)
  })
}

president_counts <- t(sapply(doc_data$Text, count_presidents, presidents = presidents))

president_data <- as.data.frame(president_counts)
president_data$Year <- doc_data$Year

president_data_long <- pivot_longer(president_data, 
                                    cols = starts_with("Biden"):starts_with("Bush"),
                                    names_to = "President", 
                                    values_to = "Mentions")

president_by_year <- president_data_long %>%
  group_by(Year, President) %>%
  summarise(TotalMentions = sum(Mentions, na.rm = TRUE))
ggplot(president_by_year, aes(x = as.numeric(Year), y = TotalMentions, color = President)) +
  geom_line() +
  geom_point() +
  labs(
    title = "Presidents' Mentions Over Time",
    x = "Year",
    y = "Total Mentions",
    color = "President"
  ) +
  theme_minimal() +
  theme(legend.position = "bottom")
```
```{r echo=FALSE, warning=FALSE}
# usage of H-1B
doc_data$H1B_Count <- sapply(doc_data$Text, function(text) {
  str_count(text, "H-1B")
})


h1b_by_year <- doc_data %>%
  group_by(Year) %>%
  summarise(TotalH1B = sum(H1B_Count, na.rm = TRUE))


ggplot(h1b_by_year, aes(x = as.numeric(Year), y = TotalH1B)) +
  geom_line() +
  geom_point() +
  labs(
    title = "H-1B Usage Over Time",
    x = "Year",
    y = "Total H-1B Mentions"
  ) +
  theme_minimal()

```


```{r stacked-bar-plot-with-regression-line, echo=FALSE, warning=FALSE}

# Count articles per year for each source
year_counts <- doc_data %>%
  group_by(Year, Source = gsub("^(NYT_|SDT_|USA_).*", "\\1", Document)) %>%
  summarise(ArticleCount = n(), .groups = 'drop')

# Convert 'Source' to a more readable format
year_counts$Source <- gsub("NYT_", "New York Times", year_counts$Source)
year_counts$Source <- gsub("SDT_", "San Diego Tribune", year_counts$Source)
year_counts$Source <- gsub("USA_", "USA Today", year_counts$Source)

# Calculate the total articles per year across all sources for the regression analysis
total_articles_per_year <- year_counts %>%
  group_by(Year) %>%
  summarise(TotalArticles = sum(ArticleCount), .groups = 'drop')

# Plot a stacked bar plot using ggplot2
ggplot() +
  geom_bar(data = year_counts, aes(x = as.numeric(Year), y = ArticleCount, fill = Source), stat = "identity", position = "stack") +
  geom_text(data = total_articles_per_year, aes(x = as.numeric(Year), y = TotalArticles, label = TotalArticles), vjust = -0.5, color = "black", size = 3) +
  geom_smooth(data = total_articles_per_year, aes(x = as.numeric(Year), y = TotalArticles), method = "lm", color = "red", se = FALSE, size = 1) +
  scale_fill_brewer(palette = "Pastel1") + # using a gentle color palette
  labs(title = "Article Count per Year from 1997-2022",
       x = "Year",
       y = "Number of Articles",
       fill = "Source") +
  theme_minimal() +
  theme(axis.text.x = element_text(angle = 45, hjust = 1)) # Improve x-axis label readability

```



```{r echo=FALSE, warning=FALSE}

# Analyze occurrences of 'not' by year
not_counts_by_year <- doc_data %>%
  mutate(Year = as.numeric(Year), NotCount = str_count(Text, "\\bnot\\b")) %>%
  group_by(Year) %>%
  summarise(TotalNotCount = sum(NotCount, na.rm = TRUE))

# Plot occurrences of 'not' by year using ggplot2
ggplot(not_counts_by_year, aes(x = Year, y = TotalNotCount)) +
  geom_col(fill = "steelblue") +
  labs(
    title = "'not' Occurrences per Year",
    x = "Year",
    y = "Occurrences"
  ) +
  theme(axis.text.x = element_text(size = 8)) # Adjust x-axis label size
```
```{r echo=FALSE, warning=FALSE}
# Step 5: Perform Sentiment Analysis
# Calculate sentiment scores for each document
doc_data$Sentiment <- sapply(doc_data$Text, function(text) {
  sentiment <- get_sentiment(text, method = "bing")
  mean(sentiment)  # Average sentiment score for the document
})
```

```{r echo=FALSE, warning=FALSE}

# Step 6: Group by Year and Analyze
# Group by year and calculate average sentiment per year
sentiment_by_year <- doc_data %>%
  group_by(Year) %>%
  summarise(AverageSentiment = mean(Sentiment, na.rm = TRUE))
```

```{r echo=FALSE, warning=FALSE}

# Step 7: Visualize Sentiment Over Time
ggplot(sentiment_by_year, aes(x = as.numeric(Year), y = AverageSentiment)) +
  geom_line() +
  geom_point() +
  labs(
    title = "Sentiment Toward Visas Over Time",
    x = "Year",
    y = "Average Sentiment Score"
  ) +
  theme_minimal()
```
```{r}
# Step 8: Validate Results
# View extracted dates, sentiment scores, and text snippets
head(doc_data[, c("Document", "Date", "Sentiment")])

# Print average sentiment by year
print(sentiment_by_year)
```
# **Next Steps**
- working analyzing text in newspapers from different years using the Term Frequency - Inverse Document Frequency (TF-IDF) method to identify trends and key terms that change over time.


# **Conclusion**
