---
title: "Project 2"
author: "Anna Brown, Rishika Deshmukh, and Ada Zhang"
date: "`r format(Sys.Date(), '%B %d, %Y')`"
output: 
  html_document:
    toc: true        # Enable the Table of Contents
    toc_depth: 3     # Set the depth of headings included in the TOC
    toc_float: true  # Enable floating TOC (keeps TOC fixed as you scroll)
    code_folding: none
    self_contained: false
    css: "styles.css"  # Link to an external stylesheet
---

```{r echo=FALSE, warning=FALSE, message=FALSE, results="hide"}
# Function to check and install missing packages
install_if_missing <- function(packages) {
  missing_packages <- packages[!packages %in% installed.packages()[, "Package"]]
  if (length(missing_packages) > 0) {
    install.packages(missing_packages, dependencies = TRUE)
  }
  invisible(lapply(packages, library, character.only = TRUE))
}

# List of required packages
packages <- c(
  "countrycode", "dplyr", "DT", "ggplot2", "gridExtra", "leaflet",
  "maps", "patchwork", "readr", "rworldmap", "showtext",
  "sp", "tidyverse", "stringr", "syuzhet"
)

# Ensure all packages are installed and loaded
install_if_missing(packages)
```

```{r include=FALSE}
# Enable the showtext package
showtext_auto()

# Add Google Fonts
font_add_google("Bodoni Moda", "bodoni_moda")  # Alias: bodoni_moda
font_add_google("Montserrat", "montserrat")     # Alias: montserrat
```

# **Introduction**  

**How does the media shape our understanding of skilled labor migration, and what do its narratives reveal about broader societal and political shifts?** The H-1B visa program has long been a cornerstone of U.S. immigration policy, sparking debates about its impact on economic growth, domestic employment, and global competitiveness. As the most heavily discussed skilled labor visa in the media, the H-1B program offers a unique lens through which to analyze the interplay between public discourse, policy decisions, and political events.  

This project examines how media coverage of H-1B visas has evolved from 1997 to 2022, focusing on the quantity and sentiment of articles from *The New York Times*, *The San Diego Union-Tribune*, and *USA Today*. We explore whether fluctuations in coverage and sentiment correlate with the number of H-1B visas issued, and how these patterns align with key political events, legislative changes, and shifts in presidential administrations. By investigating these trends, this study aims to uncover the media’s role in shaping public perceptions of skilled labor migration during pivotal moments in U.S. immigration history.  

# **Background**
The H-1B visa program allows U.S. employers to hire highly skilled foreign professionals for specialty occupations that require advanced knowledge and at least a bachelor’s degree. As the most prominent skilled labor visa, the H-1B program has played a central role in public and political discussions surrounding immigration. Its visibility in the media makes it a valuable lens through which to examine how public narratives about immigration evolve over time.

The program’s trajectory has been shaped by a combination of economic forces and political considerations. For instance, lobbying by the high-tech industry in the late 1990s led to a temporary increase in the visa cap, with the American Competitiveness in the 21st Century Act raising the annual limit to 195,000 between 2001 and 2003. However, following the 2008 recession, when protecting domestic jobs became a key legislative priority, the cap reverted to 65,000. These fluctuations underscore the program’s sensitivity to labor market conditions and broader economic trends.

Supporters of the H-1B visa highlight its role in fostering innovation, economic growth, and global competitiveness. H-1B workers are often linked to technological advancements, patents, and entrepreneurial activities that drive critical sectors such as technology, healthcare, and academia. Proponents argue that these workers help address labor shortages in high-demand fields, allowing the U.S. to remain a global leader in innovation.

Conversely, critics raise concerns about potential wage suppression and increased competition for domestic workers. They argue that poorly designed policies could disproportionately benefit corporations, potentially undermining equitable labor market outcomes. These contrasting viewpoints highlight the ongoing debate over balancing the need for international talent with the protection of domestic labor interests.

By focusing on H-1B visas, this research examines how media coverage reflects and shapes these economic debates. Understanding the narratives surrounding this program offers insights into how immigration policies balance the dual imperatives of fostering innovation and protecting domestic labor interests in an increasingly globalized economy.

# **Methodology**

## Data Sources  
This project examines news coverage of U.S. work visa programs, specifically skilled labor visas (H-1B), by analyzing articles published between January 1, 1997, and December 31, 2022, in *The New York Times*, *The San Diego Union-Tribune*, and *USA Today*.  

To construct our dataset, we used [Nexis Uni](https://advance-lexis-com.proxy1.library.virginia.edu/bisnexishome?crid=d5c0d340-3f4e-4bc3-b8f7-c2beb8781c56&pdmfid=1519360&pdisurlapi=true) to filter and extract relevant articles. The following parameters were applied to ensure the data set accurately reflects the desired coverage of H-1B visas:

| Parameter       | Description                                                      |
|-----------------|------------------------------------------------------------------|
| Keywords        | "U.S. visa", "H1-B"                                              |
| Source Type     | Newspapers                                                       |
| Source Location | North America, United States                                     |
| Language        | English                                                          |
| Timeline        | January 1, 1997 – December 31, 2022                              |
| Source Names    | *The New York Times*, *The San Diego Union-Tribune*, *USA Today* |

The filtered articles were categorized by news source and saved into text files based on their publication date. These three text files were then read into R, where they were separated into individual documents and categorized by the year they were published for further analysis. This organization allows us to track trends and sentiment shifts over time, facilitating the examination of the relationship between media coverage, policy changes, and political events.

## Limitations
A key limitation of this methodology is that Nexis Uni lacks an API for automated data extraction, requiring all articles to be manually searched, selected, and downloaded. With over 650 articles across the three datasets, human error could have led to some relevant documents being overlooked or irrelevant ones included.

To address this, we carefully defined our search parameters and conducted multiple rounds of manual checks to ensure the dataset was as accurate and relevant as possible. While this process helped minimize errors, the possibility of some inconsistencies remains. Nevertheless, we believe the dataset effectively captures the essential trends in H-1B visa coverage.

# **Exploratory Analysis**

```{r display-image, echo=FALSE}

# Use knitr to include an external image
#knitr::include_graphics("code.png")

```

```{r include=FALSE}

# Function to load, process, and write documents to separate files
load_and_process_data <- function(url, source_label) {
  content <- readLines(url, warn = FALSE)
  text <- paste(content, collapse = "\n")
  documents <- str_split(text, "End of Document", simplify = FALSE)[[1]]

  # Write each document to a separate file
  for (i in seq_along(documents)) {
    doc_path <- paste0(source_label, "_Document_", i, ".txt")
    writeLines(str_trim(documents[i]), con = doc_path)
    cat("Document saved:", doc_path, "\n")
  }
}

# Load data from New York Times
load_and_process_data("https://github.com/annab0503/DS4002/blob/main/Project%202/Skilled%20Worker%20Visas.txt?raw=true", "NYT")

# Load data from San Diego Tribune
load_and_process_data("https://github.com/annab0503/DS4002/blob/main/Project%202/.txt%20files/San%20Diego%20Tribune.txt?raw=true", "SDT")

# Load data from USA Today
load_and_process_data("https://github.com/annab0503/DS4002/blob/main/Project%202/.txt%20files/USA%20Today.txt?raw=true", "USA")


```

```{r include=FALSE}

# Step 2: Load Split Documents
doc_files <- list.files(pattern = "^(NYT_|SDT_|USA_)Document_\\d+\\.txt")

# Read all documents into a list
documents <- lapply(doc_files, readLines)

# Combine each document's content into a single text string
documents <- lapply(documents, paste, collapse = " ")

# Create a data frame for analysis
doc_data <- data.frame(
  Document = doc_files,
  Text = unlist(documents),
  stringsAsFactors = FALSE
)


```

```{r include=FALSE}


# Step 3: Extract Dates from the Text
extract_date <- function(text) {
  date_pattern <- "\\b(January|February|March|April|May|June|July|August|September|October|November|December) \\d{1,2}, \\d{4}\\b"
  date <- str_extract(text, date_pattern)
  if (is.na(date)) {
    cat("Date extraction failed for document:", substr(text, 1, 100), "...\n")
  }
  return(date)
}

# Apply the function to extract dates for all documents
doc_data$Date <- sapply(doc_data$Text, extract_date)
doc_data$Date <- as.Date(doc_data$Date, format = "%B %d, %Y")
doc_data$Year <- format(doc_data$Date, "%Y")

# Display a preview of the dataset
head(doc_data[, c("Document", "Date", "Year")])

```

```{r echo=FALSE, warning=FALSE}
library(tidyr)

#president mentions
presidents <- c("Biden", "Trump", "Obama", "Bush")


count_presidents <- function(text, presidents) {
  sapply(presidents, function(president) {
    str_count(text, president)
  })
}

president_counts <- t(sapply(doc_data$Text, count_presidents, presidents = presidents))

president_data <- as.data.frame(president_counts)
president_data$Year <- doc_data$Year

president_data_long <- pivot_longer(president_data, 
                                    cols = starts_with("Biden"):starts_with("Bush"),
                                    names_to = "President", 
                                    values_to = "Mentions")

president_by_year <- president_data_long %>%
  group_by(Year, President) %>%
  summarise(TotalMentions = sum(Mentions, na.rm = TRUE))
ggplot(president_by_year, aes(x = as.numeric(Year), y = TotalMentions, color = President)) +
  geom_line() +
  geom_point() +
  labs(
    title = "Presidents' Mentions Over Time",
    x = "Year",
    y = "Total Mentions",
    color = "President"
  ) +
  theme_minimal() +
  theme(legend.position = "bottom")
```
```{r echo=FALSE, warning=FALSE}
# usage of H-1B
doc_data$H1B_Count <- sapply(doc_data$Text, function(text) {
  str_count(text, "H-1B")
})


h1b_by_year <- doc_data %>%
  group_by(Year) %>%
  summarise(TotalH1B = sum(H1B_Count, na.rm = TRUE))


ggplot(h1b_by_year, aes(x = as.numeric(Year), y = TotalH1B)) +
  geom_line() +
  geom_point() +
  labs(
    title = "H-1B Usage Over Time",
    x = "Year",
    y = "Total H-1B Mentions"
  ) +
  theme_minimal()

```

```{r stacked-bar-plot-with-regression-line, echo=FALSE, warning=FALSE}

# Count articles per year for each source
year_counts <- doc_data %>%
  group_by(Year, Source = gsub("^(NYT_|SDT_|USA_).*", "\\1", Document)) %>%
  summarise(ArticleCount = n(), .groups = 'drop')

# Convert 'Source' to a more readable format
year_counts$Source <- gsub("NYT_", "New York Times", year_counts$Source)
year_counts$Source <- gsub("SDT_", "San Diego Tribune", year_counts$Source)
year_counts$Source <- gsub("USA_", "USA Today", year_counts$Source)

# Calculate the total articles per year across all sources for the regression analysis
total_articles_per_year <- year_counts %>%
  group_by(Year) %>%
  summarise(TotalArticles = sum(ArticleCount), .groups = 'drop')

# Plot a stacked bar plot using ggplot2
ggplot() +
  geom_bar(data = year_counts, aes(x = as.numeric(Year), y = ArticleCount, fill = Source), stat = "identity", position = "stack") +
  geom_text(data = total_articles_per_year, aes(x = as.numeric(Year), y = TotalArticles, label = TotalArticles), vjust = -0.5, color = "black", size = 3) +
  geom_smooth(data = total_articles_per_year, aes(x = as.numeric(Year), y = TotalArticles), method = "lm", color = "red", se = FALSE, size = 1) +
  scale_fill_brewer(palette = "Pastel1") + # using a gentle color palette
  labs(title = "Article Count per Year from 1997-2022",
       x = "Year",
       y = "Number of Articles",
       fill = "Source") +
  theme_minimal() +
  theme(axis.text.x = element_text(angle = 45, hjust = 1)) # Improve x-axis label readability

```

```{r echo=FALSE, warning=FALSE}

# Analyze occurrences of 'not' by year
not_counts_by_year <- doc_data %>%
  mutate(Year = as.numeric(Year), NotCount = str_count(Text, "\\bnot\\b")) %>%
  group_by(Year) %>%
  summarise(TotalNotCount = sum(NotCount, na.rm = TRUE))

# Plot occurrences of 'not' by year using ggplot2
ggplot(not_counts_by_year, aes(x = Year, y = TotalNotCount)) +
  geom_col(fill = "steelblue") +
  labs(
    title = "'not' Occurrences per Year",
    x = "Year",
    y = "Occurrences"
  ) +
  theme(axis.text.x = element_text(size = 8)) # Adjust x-axis label size
```
```{r echo=FALSE, warning=FALSE}
# Step 5: Perform Sentiment Analysis
# Calculate sentiment scores for each document
doc_data$Sentiment <- sapply(doc_data$Text, function(text) {
  sentiment <- get_sentiment(text, method = "bing")
  mean(sentiment)  # Average sentiment score for the document
})
```

```{r echo=FALSE, warning=FALSE}

# Step 6: Group by Year and Analyze
# Group by year and calculate average sentiment per year
sentiment_by_year <- doc_data %>%
  group_by(Year) %>%
  summarise(AverageSentiment = mean(Sentiment, na.rm = TRUE))
```

```{r echo=FALSE, warning=FALSE}

# Step 7: Visualize Sentiment Over Time
ggplot(sentiment_by_year, aes(x = as.numeric(Year), y = AverageSentiment)) +
  geom_line() +
  geom_point() +
  labs(
    title = "Sentiment Toward Visas Over Time",
    x = "Year",
    y = "Average Sentiment Score"
  ) +
  theme_minimal()
```
```{r}
# Step 8: Validate Results
# View extracted dates, sentiment scores, and text snippets
head(doc_data[, c("Document", "Date", "Sentiment")])

# Print average sentiment by year
print(sentiment_by_year)
```

# **Next Steps**
- working analyzing text in newspapers from different years using the Term Frequency - Inverse Document Frequency (TF-IDF) method to identify trends and key terms that change over time.

# **Conclusion**
