---
title: "Project 2"
author: "Anna Brown, Rishika Deshmukh, and Ada Zhang"
date: "`r format(Sys.Date(), '%B %d, %Y')`"
output: 
  html_document:
    toc: true        # Enable the Table of Contents
    toc_depth: 3     # Set the depth of headings included in the TOC
    toc_float: true  # Enable floating TOC (keeps TOC fixed as you scroll)
    code_folding: none
    self_contained: false
    css: "styles.css"  # Link to an external stylesheet
---

```{r echo=FALSE, warning=FALSE, message=FALSE, results="hide"}
# Function to check and install missing packages
install_if_missing <- function(packages) {
  missing_packages <- packages[!packages %in% installed.packages()[, "Package"]]
  if (length(missing_packages) > 0) {
    install.packages(missing_packages, dependencies = TRUE)
  }
  invisible(lapply(packages, library, character.only = TRUE))
}

# List of required packages
packages <- c(
  "countrycode", "dplyr", "DT", "ggplot2", "gridExtra", "leaflet",
  "maps", "patchwork", "readr", "rworldmap", "showtext",
  "sp", "tidyverse", "stringr", "syuzhet"
)

# Ensure all packages are installed and loaded
install_if_missing(packages)
```

```{r message=FALSE, include=FALSE}
# Enable the showtext package
showtext_auto()

# Add Google Fonts
font_add_google("Bodoni Moda", "bodoni_moda")  # Alias: bodoni_moda
font_add_google("Montserrat", "montserrat")     # Alias: montserrat
```
```{r fig.width=10, fig.height=6, echo=FALSE, warning=FALSE}

url <- "https://raw.githubusercontent.com/annab0503/DS4002/main/Project%201/Analysis%20Data/analysis_data.csv"

# Read the CSV file from the URL
analysis_data <- read_csv(url)

# Define key transition years in U.S. presidential administrations
transition_years <- c(2001, 2009, 2017, 2021)  # Years of presidential transitions
key_events <- c(2001, 2008, 2015, 2020)  # Key historical events

# Filter out unwanted Visa categories
filtered_data <- analysis_data %>%
  filter(!`Visa Category` %in% c("Unskilled Labor Visas", "Student and Exchange Visitor Visas"))

# Aggregate data by Fiscal Year and Visa Category
# The 'Quantity of U.S. Visas Granted' is summed and converted to thousands for clarity
aggregated_data <- filtered_data %>%
  group_by(`Fiscal Year`, `Visa Category`) %>%
  summarise(Total_Visas = sum(`Quantity of U.S. Visas Granted`) / 1000, .groups = 'drop')  # Sum the visa quantities and convert to thousands

# Custom color scheme for the different visa categories
visa_colors <- c(
  "Skilled Labor Visas" = "lightblue"  
)

# Create the stacked area chart, using the aggregated data and applying visual customization
ggplot(aggregated_data, aes(x = `Fiscal Year`, y = Total_Visas, fill = `Visa Category`)) + 
  geom_area() +  # Generate the stacked area chart to visualize the total visas over time
  geom_vline(xintercept = transition_years, linetype = "dashed", color = "gray") +  # Add vertical dashed lines at the transition years
  geom_vline(xintercept = key_events, linetype = "dashed", color = "darkred", size = 0.5) +  # Differently colored lines for key events
  scale_y_continuous(limits = c(0, 700), breaks = seq(0, 700, 100), labels = scales::comma) +  # Adjust y-axis to show 0 to 700 thousands with custom breaks
  scale_fill_manual(values = visa_colors) +  # Apply the custom color scheme for visa categories
  labs(
    title = "Shifting Borders: The Impact of U.S. Political Events on Skilled Labor Visa Issuance",  # Main title of the plot
    x = "Fiscal Year",  # X-axis label
    y = "Total Quantity of Visas (in thousands)",  # Y-axis label
    fill = "Visa Category"  # Label for the legend
  ) +
  theme_minimal() +  # Apply a minimal theme for a clean, simple appearance
  theme(
    plot.title = element_text(hjust = 0.5, family = "bodoni_moda", size = 16, margin = margin(b = 15), face = "bold"),
    legend.title = element_text(family = "bodoni_moda", size = 13, face = "bold"),
    axis.title = element_text(family = "montserrat", size = 14),
    axis.text = element_text(family = "montserrat", size = 12),
    text = element_text(size = 12, family = "montserrat"),
    legend.position = "right",
    legend.title.align = 0.5
  ) +
  # Annotate key events and presidential transitions
  annotate("text", x = key_events, y = 700, label = c("9/11 Attack", "Great Recession", "Rep. Control of Congress", "COVID Pandemic"),
           angle = 90, vjust = -0.5, hjust = 2, color = "darkred", size = 3, family = "montserrat") +
  annotate("text", x = transition_years, y = 700, label = c("Bush", "Obama", "Trump", "Biden"),
           angle = 90, vjust = -0.5, hjust = 1, color = "black", size = 3, family = "montserrat")
```

# **Introduction**  

**How does the media shape our understanding of skilled labor migration, and what do its narratives reveal about broader societal and political shifts?** The H-1B visa program has long been a cornerstone of U.S. immigration policy, sparking debates about its impact on economic growth, domestic employment, and global competitiveness. As the most heavily discussed skilled labor visa in the media, the H-1B program offers a unique lens through which to analyze the interplay between public discourse, policy decisions, and political events.  

This project examines how media coverage of H-1B visas has evolved from 1997 to 2022, focusing on the quantity and sentiment of articles from *The New York Times*, *The San Diego Union-Tribune*, and *USA Today*. We explore whether fluctuations in coverage and sentiment correlate with the number of H-1B visas issued, and how these patterns align with key political events, legislative changes, and shifts in presidential administrations. By investigating these trends, this study aims to uncover the media’s role in shaping public perceptions of skilled labor migration during pivotal moments in U.S. immigration history.  

# **Background**
The H-1B visa program allows U.S. employers to hire highly skilled foreign professionals for specialty occupations that require advanced knowledge and at least a bachelor’s degree. As the most prominent skilled labor visa, the H-1B program has played a central role in public and political discussions surrounding immigration. Its visibility in the media makes it a valuable lens through which to examine how public narratives about immigration evolve over time.

The program’s trajectory has been shaped by a combination of economic forces and political considerations. For instance, lobbying by the high-tech industry in the late 1990s led to a temporary increase in the visa cap, with the American Competitiveness in the 21st Century Act raising the annual limit to 195,000 between 2001 and 2003. However, following the 2008 recession, when protecting domestic jobs became a key legislative priority, the cap reverted to 65,000. These fluctuations underscore the program’s sensitivity to labor market conditions and broader economic trends.

Supporters of the H-1B visa highlight its role in fostering innovation, economic growth, and global competitiveness. H-1B workers are often linked to technological advancements, patents, and entrepreneurial activities that drive critical sectors such as technology, healthcare, and academia. Proponents argue that these workers help address labor shortages in high-demand fields, allowing the U.S. to remain a global leader in innovation.

Conversely, critics raise concerns about potential wage suppression and increased competition for domestic workers. They argue that poorly designed policies could disproportionately benefit corporations, potentially undermining equitable labor market outcomes. These contrasting viewpoints highlight the ongoing debate over balancing the need for international talent with the protection of domestic labor interests.

By focusing on H-1B visas, this research examines how media coverage reflects and shapes these economic debates. Understanding the narratives surrounding this program offers insights into how immigration policies balance the dual imperatives of fostering innovation and protecting domestic labor interests in an increasingly globalized economy.

# **Methodology**

## Data Sources  
This project examines news coverage of U.S. work visa programs, specifically skilled labor visas (H-1B), by analyzing articles published between January 1, 1997, and December 31, 2022, in *The New York Times*, *The San Diego Union-Tribune*, and *USA Today*.  

To construct our dataset, we used [Nexis Uni](https://advance-lexis-com.proxy1.library.virginia.edu/bisnexishome?crid=d5c0d340-3f4e-4bc3-b8f7-c2beb8781c56&pdmfid=1519360&pdisurlapi=true) to filter and extract relevant articles. The following parameters were applied to ensure the data set accurately reflects the desired coverage of H-1B visas:

| Parameter       | Description                                                      |
|-----------------|------------------------------------------------------------------|
| Keywords        | "U.S. visa", "H1-B"                                              |
| Source Type     | Newspapers                                                       |
| Source Location | North America, United States                                     |
| Language        | English                                                          |
| Timeline        | January 1, 1997 – December 31, 2022                              |
| Source Names    | *The New York Times*, *The San Diego Union-Tribune*, *USA Today* |

The filtered articles were categorized by news source and saved into text files based on their publication date. These three text files were then read into R, where they were separated into individual documents and categorized by the year they were published for further analysis. This organization allows us to track trends and sentiment shifts over time, facilitating the examination of the relationship between media coverage, policy changes, and political events.

## Limitations
A key limitation of this methodology is that Nexis Uni lacks an API for automated data extraction, requiring all articles to be manually searched, selected, and downloaded. With over 650 articles across the three datasets, human error could have led to some relevant documents being overlooked or irrelevant ones included.

To address this, we carefully defined our search parameters and conducted multiple rounds of manual checks to ensure the dataset was as accurate and relevant as possible. While this process helped minimize errors, the possibility of some inconsistencies remains. Nevertheless, we believe the dataset effectively captures the essential trends in H-1B visa coverage.

# **Exploratory Analysis**

```{r display-image, echo=FALSE}

# Use knitr to include an external image
#knitr::include_graphics("code.png")

```

```{r include=FALSE}

# Function to load, process, and write documents to separate files
load_and_process_data <- function(url, source_label) {
  content <- readLines(url, warn = FALSE)
  text <- paste(content, collapse = "\n")
  documents <- str_split(text, "End of Document", simplify = FALSE)[[1]]

  # Write each document to a separate file
  for (i in seq_along(documents)) {
    doc_path <- paste0(source_label, "_Document_", i, ".txt")
    writeLines(str_trim(documents[i]), con = doc_path)
    cat("Document saved:", doc_path, "\n")
  }
}

# Load data from New York Times
load_and_process_data("https://github.com/annab0503/DS4002/blob/main/Project%202/.txt%20files/New%20York%20Times.txt?raw=true", "NYT")

# Load data from San Diego Tribune
load_and_process_data("https://github.com/annab0503/DS4002/blob/main/Project%202/.txt%20files/San%20Diego%20Tribune.txt?raw=true", "SDT")

# Load data from USA Today
load_and_process_data("https://github.com/annab0503/DS4002/blob/main/Project%202/.txt%20files/USA%20Today.txt?raw=true", "USA")


```

```{r include=FALSE}

# Step 2: Load Split Documents
doc_files <- list.files(pattern = "^(NYT_|SDT_|USA_)Document_\\d+\\.txt")

# Read all documents into a list
documents <- lapply(doc_files, readLines)

# Combine each document's content into a single text string
documents <- lapply(documents, paste, collapse = " ")

# Create a data frame for analysis
doc_data <- data.frame(
  Document = doc_files,
  Text = unlist(documents),
  stringsAsFactors = FALSE
)


```

```{r include=FALSE}


# Step 3: Extract Dates from the Text
extract_date <- function(text) {
  date_pattern <- "\\b(January|February|March|April|May|June|July|August|September|October|November|December) \\d{1,2}, \\d{4}\\b"
  date <- str_extract(text, date_pattern)
  if (is.na(date)) {
    cat("Date extraction failed for document:", substr(text, 1, 100), "...\n")
  }
  return(date)
}

# Apply the function to extract dates for all documents
doc_data$Date <- sapply(doc_data$Text, extract_date)
doc_data$Date <- as.Date(doc_data$Date, format = "%B %d, %Y")
doc_data$Year <- format(doc_data$Date, "%Y")

# Display a preview of the dataset
head(doc_data[, c("Document", "Date", "Year")])

```

## How Has the Volume of Media Coverage on H-1B Visas Changed Over Time? What trends exist when analyzing specific keywords?


```{r echo=FALSE, warning=FALSE, message=FALSE}
library(tidyr)

#president mentions
presidents <- c("Biden", "Trump", "Obama", "Bush")


count_presidents <- function(text, presidents) {
  sapply(presidents, function(president) {
    str_count(text, president)
  })
}

president_counts <- t(sapply(doc_data$Text, count_presidents, presidents = presidents))

president_data <- as.data.frame(president_counts)
president_data$Year <- doc_data$Year

president_data_long <- pivot_longer(president_data, 
                                    cols = starts_with("Biden"):starts_with("Bush"),
                                    names_to = "President", 
                                    values_to = "Mentions")

president_by_year <- president_data_long %>%
  group_by(Year, President) %>%
  summarise(TotalMentions = sum(Mentions, na.rm = TRUE))
ggplot(president_by_year, aes(x = as.numeric(Year), y = TotalMentions, color = President)) +
  geom_line() +
  geom_point() +
  labs(
    title = "Presidents' Mentions Over Time",
    x = "Year",
    y = "Total Mentions",
    color = "President"
  ) +
  theme_minimal() +
  theme(legend.position = "bottom")
```
```{r echo=FALSE, warning=FALSE, message=FALSE}
# usage of H-1B
doc_data$H1B_Count <- sapply(doc_data$Text, function(text) {
  str_count(text, "H-1B")
})


h1b_by_year <- doc_data %>%
  group_by(Year) %>%
  summarise(TotalH1B = sum(H1B_Count, na.rm = TRUE))


ggplot(h1b_by_year, aes(x = as.numeric(Year), y = TotalH1B)) +
  geom_line() +
  geom_point() +
  labs(
    title = "H-1B Usage Over Time",
    x = "Year",
    y = "Total H-1B Mentions"
  ) +
  theme_minimal()

```

```{r stacked-bar-plot-with-regression-line, echo=FALSE, warning=FALSE, message=FALSE}

# Count articles per year for each source
year_counts <- doc_data %>%
  group_by(Year, Source = gsub("^(NYT_|SDT_|USA_).*", "\\1", Document)) %>%
  summarise(ArticleCount = n(), .groups = 'drop')

# Convert 'Source' to a more readable format
year_counts$Source <- gsub("NYT_", "New York Times", year_counts$Source)
year_counts$Source <- gsub("SDT_", "San Diego Tribune", year_counts$Source)
year_counts$Source <- gsub("USA_", "USA Today", year_counts$Source)

# Calculate the total articles per year across all sources for the regression analysis
total_articles_per_year <- year_counts %>%
  group_by(Year) %>%
  summarise(TotalArticles = sum(ArticleCount), .groups = 'drop')

# Plot a stacked bar plot using ggplot2
ggplot() +
  geom_bar(data = year_counts, aes(x = as.numeric(Year), y = ArticleCount, fill = Source), stat = "identity", position = "stack") +
  geom_text(data = total_articles_per_year, aes(x = as.numeric(Year), y = TotalArticles, label = TotalArticles), vjust = -0.5, color = "black", size = 3) +
  geom_smooth(data = total_articles_per_year, aes(x = as.numeric(Year), y = TotalArticles), method = "lm", color = "red", se = FALSE, size = 1) +
  scale_fill_brewer(palette = "Pastel1") + # using a gentle color palette
  labs(title = "Article Count per Year from 1997-2022",
       x = "Year",
       y = "Number of Articles",
       fill = "Source") +
  theme_minimal() +
  theme(axis.text.x = element_text(angle = 45, hjust = 1)) # Improve x-axis label readability

```

## How Has the Sentiment Towards H-1B Visas in Media Coverage Evolved Presidential Administrations ?

```{r echo=FALSE, warning=FALSE, message=FALSE}
# Step 5: Perform Sentiment Analysis
# Calculate sentiment scores for each document
doc_data$Sentiment <- sapply(doc_data$Text, function(text) {
  sentiment <- get_sentiment(text, method = "bing")
  mean(sentiment)  # Average sentiment score for the document
})
```

```{r echo=FALSE, warning=FALSE, message=FALSE}

# Step 6: Group by Year and Analyze
# Group by year and calculate average sentiment per year
sentiment_by_year <- doc_data %>%
  group_by(Year) %>%
  summarise(AverageSentiment = mean(Sentiment, na.rm = TRUE))
```

```{r echo=FALSE, warning=FALSE, message=FALSE}

# Step 7: Visualize Sentiment Over Time
ggplot(sentiment_by_year, aes(x = as.numeric(Year), y = AverageSentiment)) +
  geom_line() +
  geom_point() +
  labs(
    title = "Sentiment Toward Visas Over Time",
    x = "Year",
    y = "Average Sentiment Score"
  ) +
  theme_minimal()
```
```{r message=FALSE, warning=FALSE, include=FALSE}
# Step 8: Validate Results
# View extracted dates, sentiment scores, and text snippets
head(doc_data[, c("Document", "Date", "Sentiment")])

# Print average sentiment by year
print(sentiment_by_year)
```

## What Key Bigrams Dominate the Media Coverage of H-1B Visas?

```{r message=FALSE, warning=FALSE, include=FALSE}
# Load necessary libraries
library(stringr)
library(dplyr)
library(ggplot2)
library(syuzhet)
```


```{r message=FALSE, warning=FALSE, include=FALSE}


# List of news outlets to remove (adjusted for possible variations and common suffixes like "Press")
news_outlets <- c("New York Times", "San Diego Tribune", "USA Today", "Tribune", "Press", "Magazine", "New York", "San Diego")

# Initialize an empty list to store the data
all_data <- list()

# Function to load, process, and group documents by year in a DataFrame
load_and_process_data <- function(url, source_label) {
  # Try to load the content from the URL
  tryCatch({
    content <- readLines(url, warn = FALSE)
    
    # Check if content is empty or not loaded properly
    if(length(content) == 0) {
      stop("Error: The content could not be loaded or is empty.")
    }
    
    text <- paste(content, collapse = "\n")
    
    # Remove mentions of news outlets from the text
    for (outlet in news_outlets) {
      text <- str_replace_all(text, regex(paste0("\\b", outlet, "\\b"), ignore_case = TRUE), "")  # Remove the outlet name
    }
    
    # Split by "End of Document" (or another marker) to separate articles
    documents <- str_split(text, "End of Document", simplify = FALSE)[[1]]
    
    # Process each document
    articles <- list()
    for (doc in documents) {
      # Extract year from the document (assuming the year is part of the document's content)
      year_match <- regexpr("\\d{4}", doc)
      year <- ifelse(year_match != -1, substr(doc, year_match, year_match + 3), "Unknown")
      
      # Ensure extracted year is reasonable (between 1900 and current year)
      if (year != "Unknown" && (as.numeric(year) < 1900 || as.numeric(year) > as.numeric(format(Sys.Date(), "%Y")))) {
        year <- "Unknown"
      }
      
      # Append the document with its year to the articles list
      articles <- append(articles, list(data.frame(Year = year, Article = doc, stringsAsFactors = FALSE)))
    }
    
    # Combine all articles into one data frame
    df <- do.call(rbind, articles)
    
    # Combine with the global all_data list
    all_data <<- append(all_data, list(df))
    
    # Optionally, you could save this data frame to a CSV file
    write.csv(df, paste0(source_label, "_Articles_by_Year.csv"), row.names = FALSE)
    cat("Data saved to CSV:", paste0(source_label, "_Articles_by_Year.csv"), "\n")
    
  }, error = function(e) {
    cat("An error occurred while processing:", e$message, "\n")
  })
}

# Load data from New York Times
load_and_process_data("https://github.com/annab0503/DS4002/blob/main/Project%202/.txt%20files/New%20York%20Times.txt?raw=true", "NYT")

# Load data from San Diego Tribune
load_and_process_data("https://github.com/annab0503/DS4002/blob/main/Project%202/.txt%20files/San%20Diego%20Tribune.txt?raw=true", "SDT")

# Load data from USA Today
load_and_process_data("https://github.com/annab0503/DS4002/blob/main/Project%202/.txt%20files/USA%20Today.txt?raw=true", "USA")

# Combine all data into a single data frame (if needed)
all_data_combined <- do.call(rbind, all_data)


```



```{r message=FALSE, warning=FALSE, include=FALSE}
library(tidytext)
library(dplyr)
library(stringr)
library(tidyr)

                 
# List of unwanted phrases or buzzwords to remove (add more as needed)
unwanted_phrases <- c("New York Times", "San Diego", "usa today", "tribune", "magazine", "press")

# Function to clean and filter out unwanted phrases
remove_unwanted_phrases <- function(text) {
  for (phrase in unwanted_phrases) {
    text <- str_replace_all(text, fixed(phrase), "")
  }
  return(text)
}

# Function to get top bigrams used across all articles (after removing stop words, numbers, blank bigrams, and unwanted phrases)
get_top_bigrams_overall <- function(df) {
  # Tokenize the text into bigrams (pairs of consecutive words)
  bigram_count <- df %>%
    mutate(Article = sapply(Article, remove_unwanted_phrases)) %>%  # Remove unwanted phrases from articles
    unnest_tokens(bigram, Article, token = "ngrams", n = 2) %>%      # Extract bigrams
    mutate(bigram = str_replace_all(bigram, "\\d+", "")) %>%          # Remove numbers
    separate(bigram, c("word1", "word2"), sep = " ") %>%             # Separate the bigram into two words
    filter(!word1 %in% stop_words$word & !word2 %in% stop_words$word) %>%  # Remove stop words
    filter(word1 != "" & word2 != "") %>%                            # Remove blank spaces
    unite(bigram, word1, word2, sep = " ") %>%                       # Recombine the words into bigrams
    count(bigram, sort = TRUE)                                       # Count bigrams
  
  # View the top bigrams by frequency (top 50)
  top_bigrams_overall <- bigram_count %>%
    top_n(50, n) %>%  # Get the top 50 bigrams by count
    arrange(desc(n))   # Arrange in descending order
  

  return(top_bigrams_overall)  # Return the top bigrams data frame
}

# Get top bigrams used in all articles (after removing stop words, numbers, blank spaces, and unwanted phrases)
top_bigrams_all_articles <- get_top_bigrams_overall(all_data_combined)



```


```{r message=FALSE, warning=FALSE, include=FALSE}
library(tidyr)
library(tidytext)
library(dplyr)
library(stringr)

# List of unwanted phrases or buzzwords to remove (add more as needed)
unwanted_phrases <- c("New York Times", "San Diego", "usa today", "tribune", "magazine", "press")

# Function to clean and filter out unwanted phrases
remove_unwanted_phrases <- function(text) {
  for (phrase in unwanted_phrases) {
    text <- str_replace_all(text, fixed(phrase), "")
  }
  return(text)
}

# Function to calculate TF-IDF for bigrams in each year
calculate_tfidf <- function(df) {
  # Tokenize the text into bigrams (pairs of consecutive words) per year
  bigram_count <- df %>%
    mutate(Article = sapply(Article, remove_unwanted_phrases)) %>%  # Remove unwanted phrases from articles
    unnest_tokens(bigram, Article, token = "ngrams", n = 2) %>%      # Extract bigrams
    mutate(bigram = str_replace_all(bigram, "\\d+", "")) %>%          # Remove numbers
    separate(bigram, c("word1", "word2"), sep = " ") %>%             # Separate the bigram into two words
    filter(!word1 %in% stop_words$word & !word2 %in% stop_words$word) %>%  # Remove stop words
    filter(word1 != "" & word2 != "") %>%                            # Remove blank spaces
    unite(bigram, word1, word2, sep = " ")                           # Recombine the words into bigrams
  
  # Calculate Term Frequency (TF)
  tf <- bigram_count %>%
    count(Year, bigram) %>%
    group_by(Year) %>%
    mutate(TF = n / sum(n)) %>%  # Divide by total number of bigrams in the document (sum of counts)
    ungroup()

  # Calculate Document Frequency (DF)
  df_count <- bigram_count %>%
    group_by(bigram) %>%
    summarise(DF = n_distinct(Year)) %>%  # Count how many distinct years (documents) contain the bigram
    ungroup()

  # Calculate IDF
  total_docs <- n_distinct(df$Year)
  df_count <- df_count %>%
    mutate(IDF = log(total_docs / (DF + 1)))  # Log of total documents divided by the DF, added 1 to avoid division by 0

  # Filter out rows with DF = 1 or NA values
  df_count <- df_count %>%
    filter(DF > 1 & !is.na(DF))  # Remove rows with DF equal to 1 or NA values

  # Merge TF and IDF to calculate TF-IDF
  tfidf <- tf %>%
    left_join(df_count, by = "bigram") %>%
    mutate(TFIDF = TF * IDF)  # Calculate TF-IDF
  
  # Order by DF from highest to lowest
  tfidf <- tfidf %>%
    arrange(desc(DF))  # Order by DF in descending order
  
  # Return the result
  return(tfidf)
}

# Example: Apply the TF-IDF calculation to all data
tfidf_df <- calculate_tfidf(all_data_combined)





```


```{r echo=FALSE, warning=FALSE, message=FALSE}
library(tidytext)
library(dplyr)
library(stringr)
library(ggplot2)

# List of unwanted phrases or buzzwords to remove (add more as needed)
unwanted_phrases <- c("New York Times", "San Diego", "usa today", "tribune", "magazine", "press")

# Function to clean and filter out unwanted phrases
remove_unwanted_phrases <- function(text) {
  for (phrase in unwanted_phrases) {
    text <- str_replace_all(text, fixed(phrase), "")
  }
  return(text)
}

# Function to calculate TF-IDF for bigrams in each year
calculate_tfidf <- function(df) {
  # Tokenize the text into bigrams (pairs of consecutive words) per year
  bigram_count <- df %>%
    mutate(Article = sapply(Article, remove_unwanted_phrases)) %>%  # Remove unwanted phrases from articles
    unnest_tokens(bigram, Article, token = "ngrams", n = 2) %>%      # Extract bigrams
    mutate(bigram = str_replace_all(bigram, "\\d+", "")) %>%          # Remove numbers
    mutate(bigram = str_replace_all(bigram, "[[:punct:]]", "")) %>%   # Remove punctuation
    separate(bigram, c("word1", "word2"), sep = " ") %>%             # Separate the bigram into two words
    filter(!word1 %in% stop_words$word & !word2 %in% stop_words$word) %>%  # Remove stop words
    filter(word1 != "" & word2 != "") %>%                            # Remove blank spaces
    unite(bigram, word1, word2, sep = " ")   
  
  # Calculate Term Frequency (TF)
  tf <- bigram_count %>%
    count(Year, bigram) %>%
    group_by(Year) %>%
    mutate(TF = n / sum(n)) %>%  # Divide by total number of bigrams in the document (sum of counts)
    ungroup()

  # Calculate Document Frequency (DF)
  df_count <- bigram_count %>%
    group_by(bigram) %>%
    summarise(DF = n_distinct(Year)) %>%  # Count how many distinct years (documents) contain the bigram
    ungroup()

  # Calculate IDF
  total_docs <- n_distinct(df$Year)
  df_count <- df_count %>%
    mutate(IDF = log(total_docs / (DF + 1)))  # Log of total documents divided by the DF, added 1 to avoid division by 0

  # Filter out rows with DF = 1 or NA values
  df_count <- df_count %>%
    filter(DF > 1 & !is.na(DF))  # Remove rows with DF equal to 1 or NA values

  # Merge TF and IDF to calculate TF-IDF
  tfidf <- tf %>%
    left_join(df_count, by = "bigram") %>%
    mutate(TFIDF = TF * IDF)  # Calculate TF-IDF
  
  # Return the result
  return(tfidf)
}

# Calculate the average TF-IDF per year
avg_tfidf_per_year <- tfidf_df %>%
  group_by(Year) %>%
  summarise(Avg_TFIDF = mean(TFIDF, na.rm = TRUE)) %>%
  filter(Year != 1965)  # Exclude year 1965

# Plot the average TF-IDF per year
ggplot(avg_tfidf_per_year, aes(x = Year, y = Avg_TFIDF)) +
  geom_bar(stat = "identity", fill = "steelblue") +
  labs(title = "Average TF-IDF per Year (Excluding 1965)", x = "Year", y = "Average TF-IDF") +
  theme_minimal() +
  theme(
    axis.text.x = element_text(angle = 45, hjust = 1, size = 10), # Rotate x-axis labels
    axis.text.y = element_text(size = 10),  # Adjust y-axis label size
    plot.title = element_text(size = 14)   # Adjust title size
  )

```

```{r message=FALSE, warning=FALSE, include=FALSE}

library(dplyr)
library(stringr)


top_bigrams_per_year <- tfidf_df %>%
  filter(Year != 1965) %>%  # Remove year 1965
  group_by(Year) %>%
  slice_max(order_by = TFIDF, n = 5) %>%  # Select top 5 bigrams for each year based on TFIDF
  ungroup() %>%
  # Remove bigrams with punctuation
  filter(!str_detect(bigram, "[[:punct:]]"))  # Filters out bigrams that contain punctuation

top_bigrams_per_year
```
library(shiny)
library(dplyr)
library(DT)
library(shinydashboard)

# Get the top bigrams
top_bigrams_per_year <- tfidf_df %>%
  filter(Year != 1965) %>%  # Remove year 1965
  group_by(Year) %>%
  slice_max(order_by = TFIDF, n = 5) %>%  # Select top 5 bigrams for each year based on TFIDF
  ungroup()

# UI for the shiny app
ui <- dashboardPage(
  dashboardHeader(title = "Top Bigrams by Year"),
  dashboardSidebar(
    sidebarMenu(
      menuItem("Bigrams", tabName = "bigrams", icon = icon("chart-bar"))
    )
  ),
  dashboardBody(
    tabItems(
      tabItem(tabName = "bigrams",
              fluidRow(
                box(
                  title = "Select Year", status = "primary", solidHeader = TRUE,
                  selectInput("year", "Select Year:", choices = unique(top_bigrams_per_year$Year), selected = 1966)
                ),
                box(
                  title = "Top Bigrams for Selected Year", status = "primary", solidHeader = TRUE,
                  dataTableOutput("top_bigrams")
                )
              )
      )
    )
  )
)

# Server logic for the shiny app
server <- function(input, output) {
  output$top_bigrams <- renderDataTable({
    # Filter data based on the selected year and show top 5 bigrams for that year
    top_bigrams_per_year %>%
      filter(Year == input$year) %>%
      select(bigram, n, TFIDF)  # Show only the bigram, n, and TFIDF columns
  }, options = list(
    paging = TRUE,  # Enable pagination
    searching = TRUE,  # Enable search functionality
    pageLength = 5  # Show 5 rows per page
  ))
}

# Run the shiny app
shinyApp(ui = ui, server = server)

```
# **Next Steps**
- working analyzing text in newspapers from different years using the Term Frequency - Inverse Document Frequency (TF-IDF) method to identify trends and key terms that change over time.

# **Conclusion**
